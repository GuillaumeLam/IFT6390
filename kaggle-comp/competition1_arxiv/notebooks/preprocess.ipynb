{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# -update code here to with pipeline clean and preprocess stuff\n",
    "# -make class to be put in a python file with the same code as here and call those methods in other notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/gui/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/gui/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/gui/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../dataset/train.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    data = list(csv_reader)\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7501, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data[1:,1]\n",
    "arxiv_label = data[1:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('  We report on the detection of dark matter in the cluster Abell 2218 using the\\nweak gravitational distortion of background galaxies. We find a highly\\nsignificant, coherent detection of the distortion in the images of the\\nbackground galaxies. The inferred 2D mass distribution has a peak that is\\ncoincident with the optical and X-ray centroid. The qualitative distributions\\nof the cluster light, the X-ray emission and the dark matter are similar and\\nthe projected total mass, gas, and light surface densities are consistent with\\na $r^{-1}$ profile at distance of $r > 180^{\\\\prime\\\\prime}$ from the cluster cD\\ngalaxy. Using the weak lensing technique, we determine a lower bound for the\\ntotal mass in A2218 of $(3.9 \\\\pm 0.7) \\\\times 10^{14}$~h$^{-1}$~M$_\\\\odot$ within\\na fiducial aperture of radius 0.4~h$^{-1}$Mpc. The associated cluster\\nmass-to-light ratio is $(440 \\\\pm 80)$~h~$M_\\\\odot/L_{\\\\odot B}$. The mass\\nestimated by the weak lensing method is consistent with that inferred from the\\nX-ray data under the assumption of hydrostatic equilibrium and we derive an\\nupper bound for the gas-to-total mass ratio at 400~h$^{-1}$kpc of\\n$M_{gas}/M_{tot} = (0.04 \\\\pm 0.02)$h$^{-3/2}$.\\n',\n",
       " 'astro-ph')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n= 1340\n",
    "text[n], arxiv_label[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = np.unique(arxiv_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['astro-ph', 'astro-ph.CO', 'astro-ph.GA', 'astro-ph.SR',\n",
       "       'cond-mat.mes-hall', 'cond-mat.mtrl-sci', 'cs.LG', 'gr-qc',\n",
       "       'hep-ph', 'hep-th', 'math.AP', 'math.CO', 'physics.optics',\n",
       "       'quant-ph', 'stat.ML'], dtype='<U2273')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = {}\n",
    "for _,_, cat in data[1:]:\n",
    "    if cat in dist:\n",
    "        dist[cat] += 1\n",
    "    else:\n",
    "        dist[cat] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'astro-ph': 500,\n",
       " 'astro-ph.CO': 500,\n",
       " 'astro-ph.GA': 500,\n",
       " 'astro-ph.SR': 500,\n",
       " 'cond-mat.mes-hall': 500,\n",
       " 'cond-mat.mtrl-sci': 500,\n",
       " 'cs.LG': 500,\n",
       " 'gr-qc': 500,\n",
       " 'hep-ph': 500,\n",
       " 'hep-th': 500,\n",
       " 'math.AP': 500,\n",
       " 'math.CO': 500,\n",
       " 'physics.optics': 500,\n",
       " 'quant-ph': 500,\n",
       " 'stat.ML': 500}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of word types (nouns and adjectives) to leave in the text\n",
    "defTags = ['NN', 'NNS', 'NNP', 'NNPS', 'JJ', 'JJS', 'JJR']#, 'RB', 'RBS', 'RBR', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "# functions to determine the type of a word\n",
    "def is_noun(tag):\n",
    "    return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "\n",
    "\n",
    "def is_verb(tag):\n",
    "    return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "\n",
    "def is_adverb(tag):\n",
    "    return tag in ['RB', 'RBR', 'RBS']\n",
    "\n",
    "\n",
    "def is_adjective(tag):\n",
    "    return tag in ['JJ', 'JJR', 'JJS']\n",
    "\n",
    "# transform tag forms\n",
    "def penn_to_wn(tag):\n",
    "    if is_adjective(tag):\n",
    "        return nltk.stem.wordnet.wordnet.ADJ\n",
    "    elif is_noun(tag):\n",
    "        return nltk.stem.wordnet.wordnet.NOUN\n",
    "    elif is_adverb(tag):\n",
    "        return nltk.stem.wordnet.wordnet.ADV\n",
    "    elif is_verb(tag):\n",
    "        return nltk.stem.wordnet.wordnet.VERB\n",
    "    return nltk.stem.wordnet.wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(comment_string, lemmatizer):\n",
    "        clean_tokens = []\n",
    "\n",
    "        token = comment_string\n",
    "        token = re.sub(r'\\b(?:(?:https?|ftp):\\/\\/)?\\w[\\w-]*(?:\\.[\\w-]+)+\\S*', lambda x:' ' + re.findall('\\w[\\w-]*(?:\\.[\\w-]+)+', x.group())[0], token)\n",
    "        token = re.sub('r/', ' ', token)\n",
    "        token = re.sub('u/', ' ', token)\n",
    "        \n",
    "        \n",
    "        token = re.sub(r'\\[', ' ', token)\n",
    "        token = re.sub(r'\\]', ' ', token)\n",
    "        \n",
    "        token = re.sub(r'\\dv\\d', 'vs', token)\n",
    "        token = re.sub(r'\\n', ' ', token)\n",
    "\n",
    "\n",
    "        token = re.sub(r'\\?', ' ', token)\n",
    "        token = re.sub(r'\\\"', ' ', token)\n",
    "\n",
    "        token = re.sub(r'\\!', ' ', token)\n",
    "        token = re.sub(r'\\,', ' ', token)\n",
    "        token = re.sub(r'\\.', ' ', token)\n",
    "        token = re.sub(r'\\:', ' ', token)\n",
    "        token = re.sub(r'\\;', ' ', token)\n",
    "        token = re.sub(r'\\)', ' ', token)\n",
    "        token = re.sub(r'\\(', ' ', token)\n",
    "        token = re.sub(r'\\\"', ' ', token)\n",
    "        token = re.sub(r\"\\'\", ' ', token)\n",
    "        token = re.sub(r'\\+', ' ', token)\n",
    "        token = re.sub(r\"\\-\", ' ', token)\n",
    "        token = re.sub(r\"\\~\", ' ', token)\n",
    "        token = re.sub(r\"\\*\", ' ', token)\n",
    "        token = re.sub(r\"\\&\", ' ', token)\n",
    "        token = re.sub(r\"\\{\", ' ', token)\n",
    "        token = re.sub(r\"\\}\", ' ', token)\n",
    "        token = re.sub(r\"\\|\", ' ', token)\n",
    "        token = re.sub(r\"\\/\", ' ', token)\n",
    "        token = re.sub(r\"\\#\", ' # ', token)\n",
    "        token = re.sub(' +', ' ', token)\n",
    "\n",
    "        for thing, tag in nltk.pos_tag(token.split()):\n",
    "            if (thing not in string.punctuation):\n",
    "                clean_tokens.append(lemmatizer.lemmatize(thing, penn_to_wn(tag)))\n",
    "\n",
    "        token = ' '.join(clean_tokens)\n",
    "    \n",
    "        token = re.sub(r' \\d+\\$', ' [money]', token)\n",
    "        token = re.sub(r' \\$\\d+', ' [money]', token)\n",
    "        token = re.sub(r' \\d+M', ' [money]', token)\n",
    "        token = re.sub(r' M\\d+', ' [money]', token)\n",
    "\n",
    "        token = re.sub(r' \\d+m', ' [distance]', token)\n",
    "        token = re.sub(r' \\d+km', ' [distance]', token)\n",
    "        token = re.sub(r' \\d+KM', ' [distance]', token)\n",
    "        token = re.sub(r' \\d+cm', ' [distance]', token)\n",
    "        \n",
    "        token = re.sub(r' \\d+pm', ' [time]', token)\n",
    "        token = re.sub(r' \\d+PM', ' [time]', token)\n",
    "        token = re.sub(r' \\d+am', ' [time]', token)\n",
    "        token = re.sub(r' \\d+AM', ' [time]', token)\n",
    "  \n",
    "  \n",
    "        token = re.sub(r' 200\\d ', ' [year]', token)\n",
    "        token = re.sub(r' 20\\d\\d ', ' [year]', token)\n",
    "        token = re.sub(r' 199\\d ', ' [year]', token)\n",
    "\n",
    "      \n",
    "        token = re.sub(r' \\d+', ' [number] ', token)\n",
    "\n",
    "        token = token.lower()\n",
    "\n",
    "        token = re.sub(r'\\_', ' ', token)\n",
    "    \n",
    "    \n",
    "        return ' '.join(token.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a recent paper demonstrate a considerable degree of self similarity between rr lyrae star and their atomic scale analogue excite helium atom undergo single level transition between n [number] and n [number] discrete self similarity between these fractal analogue be indentified in term of their mass radii oscillation period basic morphology and kinematics in this second paper on the subject an extremely large and carefully analyzed sample of rr lyrae oscillation period provide further evidence for a unique match between the predicted set of discrete period base exclusively on the known helium spectrum and the discrete scaling equation of a fractal cosmological paradigm and the observed period spectrum of rr lyrae star astro-ph\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "n = 3933\n",
    "print(clean(text[n], lemmatizer), arxiv_label[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(file_path, data = 'train', vectorizer = 'tfidf', max_features = 20000):    \n",
    "    ARXIV = ['astro-ph', 'astro-ph.CO', 'astro-ph.GA', 'astro-ph.SR',\n",
    "       'cond-mat.mes-hall', 'cond-mat.mtrl-sci', 'cs.LG', 'gr-qc',\n",
    "       'hep-ph', 'hep-th', 'math.AP', 'math.CO', 'physics.optics',\n",
    "       'quant-ph', 'stat.ML']\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    if vectorizer == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer(max_features = max_features)\n",
    "    elif vectorizer == 'count':\n",
    "        vectorizer = CountVectorizer(max_features = max_features)\n",
    "    elif vectorizer == 'binary':\n",
    "        vectorizer = CountVectorizer(max_features = max_features, binary = True)\n",
    "\n",
    "\n",
    "    with open(file_path) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        colnames = next(csv_reader)\n",
    "\n",
    "        print('cleaning...')\n",
    "        if data == 'train':\n",
    "            raw_data = [[_, clean(comment, lemmatizer), ARXIV.index(cl)] for _, comment, cl in list(csv_reader)]\n",
    "            X, y = np.array(raw_data)[:, 1], np.array(raw_data)[:, 2]\n",
    "            \n",
    "        elif data == 'test':\n",
    "            raw_data = [[_, clean(comment, lemmatizer)] for _, comment in list(csv_reader)]\n",
    "            X, y = np.array(raw_data)[:, 1], None\n",
    "\n",
    "    if data == 'train':\n",
    "        print('splitting data...')\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=11)\n",
    "\n",
    "        print('vectorizing...')\n",
    "        X_train = vectorizer.fit_transform(X_train).toarray()\n",
    "        X_val = vectorizer.transform(X_val).toarray()\n",
    "        \n",
    "        print('done!')\n",
    "        return X_train, X_val, y_train, y_val, vectorizer\n",
    "      \n",
    "    elif data == 'test':\n",
    "        print('vectorizing...')\n",
    "        X = vectorizer.transform(X).toarray()\n",
    "\n",
    "        print('done!')\n",
    "        return X, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning...\n",
      "splitting data...\n",
      "vectorizing...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val, vectorizer = pre_process('../dataset/train.csv', data = 'train', vectorizer = 'tfidf', max_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['13', '13', '3', ..., '0', '14', '3'], dtype='<U2218')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-virtualenv-name",
   "language": "python",
   "name": "my-virtualenv-name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
