{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/gui/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/gui/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/gui/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../dataset/train.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    data = list(csv_reader)\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7501, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data[1:,1]\n",
    "arxiv_label = data[1:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('  We report on the detection of dark matter in the cluster Abell 2218 using the\\nweak gravitational distortion of background galaxies. We find a highly\\nsignificant, coherent detection of the distortion in the images of the\\nbackground galaxies. The inferred 2D mass distribution has a peak that is\\ncoincident with the optical and X-ray centroid. The qualitative distributions\\nof the cluster light, the X-ray emission and the dark matter are similar and\\nthe projected total mass, gas, and light surface densities are consistent with\\na $r^{-1}$ profile at distance of $r > 180^{\\\\prime\\\\prime}$ from the cluster cD\\ngalaxy. Using the weak lensing technique, we determine a lower bound for the\\ntotal mass in A2218 of $(3.9 \\\\pm 0.7) \\\\times 10^{14}$~h$^{-1}$~M$_\\\\odot$ within\\na fiducial aperture of radius 0.4~h$^{-1}$Mpc. The associated cluster\\nmass-to-light ratio is $(440 \\\\pm 80)$~h~$M_\\\\odot/L_{\\\\odot B}$. The mass\\nestimated by the weak lensing method is consistent with that inferred from the\\nX-ray data under the assumption of hydrostatic equilibrium and we derive an\\nupper bound for the gas-to-total mass ratio at 400~h$^{-1}$kpc of\\n$M_{gas}/M_{tot} = (0.04 \\\\pm 0.02)$h$^{-3/2}$.\\n',\n",
       " 'astro-ph')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n= 1340\n",
    "text[n], arxiv_label[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = np.unique(arxiv_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['astro-ph', 'astro-ph.CO', 'astro-ph.GA', 'astro-ph.SR',\n",
       "       'cond-mat.mes-hall', 'cond-mat.mtrl-sci', 'cs.LG', 'gr-qc',\n",
       "       'hep-ph', 'hep-th', 'math.AP', 'math.CO', 'physics.optics',\n",
       "       'quant-ph', 'stat.ML'], dtype='<U2273')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = {}\n",
    "for _,_, cat in data[1:]:\n",
    "    if cat in dist:\n",
    "        dist[cat] += 1\n",
    "    else:\n",
    "        dist[cat] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'astro-ph': 500,\n",
       " 'astro-ph.CO': 500,\n",
       " 'astro-ph.GA': 500,\n",
       " 'astro-ph.SR': 500,\n",
       " 'cond-mat.mes-hall': 500,\n",
       " 'cond-mat.mtrl-sci': 500,\n",
       " 'cs.LG': 500,\n",
       " 'gr-qc': 500,\n",
       " 'hep-ph': 500,\n",
       " 'hep-th': 500,\n",
       " 'math.AP': 500,\n",
       " 'math.CO': 500,\n",
       " 'physics.optics': 500,\n",
       " 'quant-ph': 500,\n",
       " 'stat.ML': 500}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to determine the type of a word\n",
    "def is_noun(tag):\n",
    "    return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "\n",
    "\n",
    "def is_verb(tag):\n",
    "    return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "\n",
    "def is_adverb(tag):\n",
    "    return tag in ['RB', 'RBR', 'RBS']\n",
    "\n",
    "\n",
    "def is_adjective(tag):\n",
    "    return tag in ['JJ', 'JJR', 'JJS']\n",
    "\n",
    "# transform tag forms\n",
    "def penn_to_wn(tag):\n",
    "    if is_adjective(tag):\n",
    "        return nltk.stem.wordnet.wordnet.ADJ\n",
    "    elif is_noun(tag):\n",
    "        return nltk.stem.wordnet.wordnet.NOUN\n",
    "    elif is_adverb(tag):\n",
    "        return nltk.stem.wordnet.wordnet.ADV\n",
    "    elif is_verb(tag):\n",
    "        return nltk.stem.wordnet.wordnet.VERB\n",
    "    return nltk.stem.wordnet.wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(comment_string, lemmatizer):\n",
    "        clean_tokens = []\n",
    "\n",
    "        token = comment_string\n",
    "        \n",
    "        for thing, tag in nltk.pos_tag(token.split()):\n",
    "            if (thing not in string.punctuation):\n",
    "                clean_tokens.append(lemmatizer.lemmatize(thing, penn_to_wn(tag)))\n",
    "        \n",
    "        clean_tokens = [word for word in clean_tokens if word not in stopwords.words('english')]\n",
    "            \n",
    "        token = ' '.join(clean_tokens)\n",
    "                \n",
    "        matches = re.findall(r'\\\\\\w*', token)\n",
    "        matches = [re.sub(r'\\\\', '', word) for word in list(set(matches))]\n",
    "        token = re.sub(r'\\\\\\w*', ' ', token)\n",
    "        token = token + ' '.join(matches)\n",
    "        \n",
    "        token = re.sub(r'\\[', ' ', token)\n",
    "        token = re.sub(r'\\]', ' ', token)\n",
    "\n",
    "        \n",
    "        token = re.sub(r'\\n', ' ', token)\n",
    "\n",
    "        token = re.sub(r'\\?', ' ', token)\n",
    "        token = re.sub(r'\\\"', ' ', token)\n",
    "        token = re.sub(r'\\!', ' ', token)\n",
    "        token = re.sub(r'\\,', ' ', token)\n",
    "        token = re.sub(r'\\.', ' ', token)\n",
    "        token = re.sub(r'\\:', ' ', token)\n",
    "        token = re.sub(r'\\;', ' ', token)\n",
    "        token = re.sub(r'\\)', ' ', token)\n",
    "        token = re.sub(r'\\(', ' ', token)\n",
    "\n",
    "        token = re.sub(r\"\\'\", ' ', token)\n",
    "        token = re.sub(r'\\+', ' ', token)\n",
    "        token = re.sub(r\"\\-\", ' ', token)\n",
    "        token = re.sub(r\"\\~\", ' ', token)\n",
    "        token = re.sub(r\"\\*\", ' ', token)\n",
    "        token = re.sub(r\"\\&\", ' ', token)\n",
    "        token = re.sub(r\"\\{\", ' ', token)\n",
    "        token = re.sub(r\"\\}\", ' ', token)\n",
    "        token = re.sub(r\"\\|\", ' ', token)\n",
    "        token = re.sub(r\"\\/\", ' ', token)\n",
    "        token = re.sub(r\"\\#\", ' # ', token)\n",
    "        token = re.sub(' +', ' ', token)\n",
    "\n",
    "        token = re.sub(r' 200\\d ', ' [year]', token)\n",
    "        token = re.sub(r' 20\\d\\d ', ' [year]', token)\n",
    "        token = re.sub(r' 199\\d ', ' [year]', token)\n",
    "\n",
    "        token = re.sub(r' \\d+', ' [number] ', token)\n",
    "\n",
    "        token = token.lower()\n",
    "        \n",
    "#         token = re.sub(r'\\$( )*\\$', ' ', token)\n",
    "# \n",
    "#         token = re.sub(r'\\$.*\\$', ' ', token)\n",
    "# \n",
    "\n",
    "        token = re.sub(r'\\_', ' ', token)\n",
    "\n",
    "        return ' '.join(token.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a recent paper demonstrate considerable degree self similarity rr lyrae star atomic scale analogues excite helium atom undergo single level transition n [number] n [number] discrete self similarity fractal analogue indentified term masses radii oscillation periods basic morphology kinematics in second paper subject extremely large carefully analyzed sample rr lyrae oscillation period provide evidence unique match predicted set discrete periods base exclusively known helium spectrum discrete scaling equation fractal cosmological paradigm observed period spectrum rr lyrae stars astro-ph\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "n = 3933\n",
    "print(clean(text[n], lemmatizer), arxiv_label[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(file_path, data = 'train', vectorizer = 'tfidf', max_features = None, existing_vectorizer=None):    \n",
    "    ARXIV = ['astro-ph', 'astro-ph.CO', 'astro-ph.GA', 'astro-ph.SR',\n",
    "       'cond-mat.mes-hall', 'cond-mat.mtrl-sci', 'cs.LG', 'gr-qc',\n",
    "       'hep-ph', 'hep-th', 'math.AP', 'math.CO', 'physics.optics',\n",
    "       'quant-ph', 'stat.ML']\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    if existing_vectorizer:\n",
    "        vectorizer = existing_vectorizer\n",
    "    else:\n",
    "        if vectorizer == 'tfidf':\n",
    "            vectorizer = TfidfVectorizer(max_features = max_features)\n",
    "        elif vectorizer == 'count':\n",
    "            vectorizer = CountVectorizer(max_features = max_features)\n",
    "        elif vectorizer == 'binary':\n",
    "            vectorizer = CountVectorizer(max_features = max_features, binary = True)\n",
    "\n",
    "\n",
    "    with open(file_path) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        colnames = next(csv_reader)\n",
    "\n",
    "        print('cleaning...')\n",
    "        if data == 'train':\n",
    "            raw_data = [[_, clean(comment, lemmatizer), ARXIV.index(cl)] for _, comment, cl in list(csv_reader)]\n",
    "            X, y = np.array(raw_data)[:, 1], np.array(raw_data)[:, 2]\n",
    "            \n",
    "        elif data == 'test':\n",
    "            raw_data = [[_, clean(comment, lemmatizer)] for _, comment in list(csv_reader)]\n",
    "            X, y = np.array(raw_data)[:, 1], None\n",
    "\n",
    "    if data == 'train':\n",
    "\n",
    "        print('vectorizing...')\n",
    "        X = vectorizer.fit_transform(X).toarray()\n",
    "        \n",
    "        print('done!')\n",
    "        return X, y, vectorizer\n",
    "      \n",
    "    elif data == 'test':\n",
    "        print('vectorizing...')\n",
    "        X = vectorizer.transform(X).toarray()\n",
    "\n",
    "        print('done!')\n",
    "        return X, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning...\n",
      "vectorizing...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "X, y, vectorizer = pre_process('../dataset/train.csv', data = 'train', vectorizer = 'tfidf', max_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0', '8', '6', ..., '9', '11', '9'], dtype='<U1807')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-virtualenv-name",
   "language": "python",
   "name": "my-virtualenv-name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
