{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# -clean up code and make python classes/methods to be importe and called simply here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/gui/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/gui/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/gui/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# from src.preprocess import pre_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of word types (nouns and adjectives) to leave in the text\n",
    "defTags = ['NN', 'NNS', 'NNP', 'NNPS', 'JJ', 'JJS', 'JJR']#, 'RB', 'RBS', 'RBR', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "# functions to determine the type of a word\n",
    "def is_noun(tag):\n",
    "    return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "\n",
    "\n",
    "def is_verb(tag):\n",
    "    return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "\n",
    "def is_adverb(tag):\n",
    "    return tag in ['RB', 'RBR', 'RBS']\n",
    "\n",
    "\n",
    "def is_adjective(tag):\n",
    "    return tag in ['JJ', 'JJR', 'JJS']\n",
    "\n",
    "# transform tag forms\n",
    "def penn_to_wn(tag):\n",
    "    if is_adjective(tag):\n",
    "        return nltk.stem.wordnet.wordnet.ADJ\n",
    "    elif is_noun(tag):\n",
    "        return nltk.stem.wordnet.wordnet.NOUN\n",
    "    elif is_adverb(tag):\n",
    "        return nltk.stem.wordnet.wordnet.ADV\n",
    "    elif is_verb(tag):\n",
    "        return nltk.stem.wordnet.wordnet.VERB\n",
    "    return nltk.stem.wordnet.wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(comment_string, lemmatizer):\n",
    "        clean_tokens = []\n",
    "        replacement_count = []\n",
    "\n",
    "        \n",
    "        token = comment_string\n",
    "        \n",
    "        for thing, tag in nltk.pos_tag(token.split()):\n",
    "            if (thing not in string.punctuation):\n",
    "                clean_tokens.append(lemmatizer.lemmatize(thing, penn_to_wn(tag)))\n",
    "        \n",
    "        clean_tokens = [word for word in clean_tokens if word not in stopwords.words('english')]\n",
    "            \n",
    "        token = ' '.join(clean_tokens)\n",
    "                \n",
    "        matches = re.findall(r'\\\\\\w*', token)\n",
    "        matches = [re.sub(r'\\\\', '', word) for word in list(set(matches))]\n",
    "        token = re.sub(r'\\\\\\w*', ' ', token)\n",
    "        \n",
    "        token = token + ' '.join(matches)\n",
    "        \n",
    "        token = re.sub(r'\\[', ' ', token)\n",
    "\n",
    "        token = re.sub(r'\\]', ' ', token)\n",
    "\n",
    "        \n",
    "        token = re.sub(r'\\n', ' ', token)\n",
    "\n",
    "\n",
    "\n",
    "        token = re.sub(r'\\?', ' ', token)\n",
    "\n",
    "        token = re.sub(r'\\\"', ' ', token)\n",
    "\n",
    "\n",
    "        token = re.sub(r'\\!', ' ', token)\n",
    "\n",
    "        token = re.sub(r'\\,', ' ', token)\n",
    "\n",
    "        token = re.sub(r'\\.', ' ', token)\n",
    "\n",
    "        token = re.sub(r'\\:', ' ', token)\n",
    "\n",
    "        token = re.sub(r'\\;', ' ', token)\n",
    "\n",
    "        token = re.sub(r'\\)', ' ', token)\n",
    "\n",
    "        token = re.sub(r'\\(', ' ', token)\n",
    "\n",
    "\n",
    "        token = re.sub(r\"\\'\", ' ', token)\n",
    "\n",
    "        token = re.sub(r'\\+', ' ', token)\n",
    "\n",
    "        token = re.sub(r\"\\-\", ' ', token)\n",
    "\n",
    "        token = re.sub(r\"\\~\", ' ', token)\n",
    "\n",
    "        token = re.sub(r\"\\*\", ' ', token)\n",
    "\n",
    "        token = re.sub(r\"\\&\", ' ', token)\n",
    "\n",
    "        token = re.sub(r\"\\{\", ' ', token)\n",
    "\n",
    "        token = re.sub(r\"\\}\", ' ', token)\n",
    "\n",
    "        token = re.sub(r\"\\|\", ' ', token)\n",
    "\n",
    "        token = re.sub(r\"\\/\", ' ', token)\n",
    "\n",
    "        token = re.sub(r\"\\#\", ' # ', token)\n",
    "\n",
    "        token = re.sub(' +', ' ', token)\n",
    "\n",
    "    \n",
    "    \n",
    "        token = re.sub(r' 200\\d ', ' [year]', token)\n",
    "\n",
    "        token = re.sub(r' 20\\d\\d ', ' [year]', token)\n",
    "\n",
    "        token = re.sub(r' 199\\d ', ' [year]', token)\n",
    "\n",
    "\n",
    "      \n",
    "        token = re.sub(r' \\d+', ' [number] ', token)\n",
    "\n",
    "\n",
    "        token = token.lower()\n",
    "        \n",
    "#         token = re.sub(r'\\$( )*\\$', ' ', token)\n",
    "# \n",
    "        \n",
    "#         token = re.sub(r'\\$.*\\$', ' ', token)\n",
    "# \n",
    "\n",
    "        token = re.sub(r'\\_', ' ', token)\n",
    "\n",
    "            \n",
    "        return ' '.join(token.split()), replacement_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(file_path, data = 'train', vectorizer = 'tfidf', max_features = None, existing_vectorizer=None):    \n",
    "    ARXIV = ['astro-ph', 'astro-ph.CO', 'astro-ph.GA', 'astro-ph.SR',\n",
    "       'cond-mat.mes-hall', 'cond-mat.mtrl-sci', 'cs.LG', 'gr-qc',\n",
    "       'hep-ph', 'hep-th', 'math.AP', 'math.CO', 'physics.optics',\n",
    "       'quant-ph', 'stat.ML']\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    if existing_vectorizer:\n",
    "        vectorizer = existing_vectorizer\n",
    "    else:\n",
    "        if vectorizer == 'tfidf':\n",
    "            vectorizer = TfidfVectorizer(max_features = max_features)\n",
    "        elif vectorizer == 'count':\n",
    "            vectorizer = CountVectorizer(max_features = max_features)\n",
    "        elif vectorizer == 'binary':\n",
    "            vectorizer = CountVectorizer(max_features = max_features, binary = True)\n",
    "\n",
    "\n",
    "    with open(file_path) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        colnames = next(csv_reader)\n",
    "\n",
    "        print('cleaning...')\n",
    "        if data == 'train':\n",
    "            raw_data = [[_, clean(comment, lemmatizer)[0], ARXIV.index(cl)] for _, comment, cl in list(csv_reader)]\n",
    "            X, y = np.array(raw_data)[:, 1], np.array(raw_data)[:, 2]\n",
    "            \n",
    "        elif data == 'test':\n",
    "            raw_data = [[_, clean(comment, lemmatizer)[0]] for _, comment in list(csv_reader)]\n",
    "            X, y = np.array(raw_data)[:, 1], None\n",
    "\n",
    "    if data == 'train':\n",
    "\n",
    "        print('vectorizing...')\n",
    "        X = vectorizer.fit_transform(X).toarray()\n",
    "        \n",
    "        print('done!')\n",
    "        return X, y, vectorizer\n",
    "      \n",
    "    elif data == 'test':\n",
    "        print('vectorizing...')\n",
    "        X = vectorizer.transform(X).toarray()\n",
    "\n",
    "        print('done!')\n",
    "        return X, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning...\n",
      "vectorizing...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# X_train, X_val, y_train, y_val, vectorizer = pre_process('dataset/train.csv',\n",
    "#                                                          data = 'train',\n",
    "#                                                          vectorizer = 'tfidf',\n",
    "#                                                         max_features=None)\n",
    "# X = np.concatenate((X_train,X_val), axis=0)\n",
    "# y = np.concatenate((y_train,y_val), axis=0)\n",
    "\n",
    "X, y, vectorizer = pre_process('dataset/train.csv',\n",
    "                                data = 'train',\n",
    "                                vectorizer = 'tfidf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliNB:\n",
    "\n",
    "    def __init__(self, alpha=1):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        n_classes = self.n_classes\n",
    "\n",
    "        # calculate P(C_k) for all k\n",
    "        self.counts = np.zeros(n_classes)\n",
    "        for i in y:\n",
    "            self.counts[int(i)] += 1\n",
    "        self.counts /= len(y)\n",
    "\n",
    "        # generate n_features x n_classes matrix\n",
    "        self.params = np.zeros((n_classes, X.shape[1]))\n",
    "        for idx in range(len(X)):\n",
    "            self.params[int(y[idx])] += X[idx]\n",
    "        self.params += self.alpha #1 # Laplace\n",
    "\n",
    "        # This is the correct thing to do\n",
    "        self.class_sums = np.zeros(self.n_classes)\n",
    "        for i in y:\n",
    "            self.class_sums[int(i)] += 1\n",
    "        self.class_sums += self.n_classes*self.alpha # Laplace\n",
    "\n",
    "        # Now our algorithm is happy\n",
    "        self.params = self.params / self.class_sums[:, np.newaxis]\n",
    "\n",
    "    def predict(self, X):\n",
    "        neg_prob = np.log(1 - self.params)\n",
    "        # Compute  neg_prob · (1 - X).T  as  ∑neg_prob - X · neg_prob\n",
    "        jll = np.dot(X, (np.log(self.params) - neg_prob).T)\n",
    "        jll += np.log(self.counts) + neg_prob.sum(axis=1)\n",
    "        return np.argmax(jll, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7500, 25092)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X.shape)\n",
    "bnb = MultinomialNB(0.1)\n",
    "bnb.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning...\n",
      "vectorizing...\n",
      "done!\n",
      "(15000, 25092)\n",
      "[14  3  3 ...  0  7  4]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X, _ = pre_process('dataset/test.csv',\n",
    "                    data = 'test',\n",
    "                    vectorizer = 'tfidf',\n",
    "                    existing_vectorizer = vectorizer)\n",
    "print(X.shape)\n",
    "y_pred = bnb.predict(X).astype(int)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gui/.local/share/virtualenvs/competition1-arxiv-0Jd2s1wY/lib/python3.5/site-packages/ipykernel_launcher.py:12: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "ARXIV = ['astro-ph', 'astro-ph.CO', 'astro-ph.GA', 'astro-ph.SR',\n",
    "       'cond-mat.mes-hall', 'cond-mat.mtrl-sci', 'cs.LG', 'gr-qc',\n",
    "       'hep-ph', 'hep-th', 'math.AP', 'math.CO', 'physics.optics',\n",
    "       'quant-ph', 'stat.ML']\n",
    "\n",
    "df = pd.DataFrame(data = y_pred)\n",
    "\n",
    "df.columns = ['Category']\n",
    "df['Id'] = np.arange(len(df))\n",
    "df = df.apply (lambda row: ARXIV[int(row['Category'])], axis=1)\n",
    "df.columns = ['Id', 'Category']\n",
    "df.to_csv('MNB_gridsearch_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-virtualenv-name",
   "language": "python",
   "name": "my-virtualenv-name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
